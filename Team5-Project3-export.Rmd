---
title: "Team 5 - Project 3"
author: "Ariba Mandavia, Jose Fuentes, Marco Castro, Steven Gonzalez"
date: "2024-10-27"
output: html_document
---

## Overview / Introduction

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(dbplyr)
library(tidyr)
library(tibble)
library(rvest)
library(rlist)
library(readr)
library(XML)
library(xml2)
library(jsonlite)
library(arrow)
library(stringr)
library(digest)
library(duckdb)
library(ggplot2)
```

## Overview 

For Project 3, our team was asked to answer the question, _Which are the most valued data science skills?_ To answer this question, we decided to explore the top skills that companies search for in a job candidate. We sourced data from [Indeed](https://indeed.com/), a popular job listing site. We were able to source a [dataset](https://www.kaggle.com/datasets/asaniczka/data-science-job-postings-and-skills) featuring over 12,000 US Data Science jobs scraped from Indeed in January 2024<sup>*</sup>.

We began by normalizing the dataset to match our proposed Entity Relationship Diagram (ERD) shown below:

![Entity Relationship Diagram](ERD.png). 

We have broken out our normalization process int a [separate document](https://github.com/mcastro64/D607_Project3/blob/bcdc0748160cd2e9abca7ece7aff03f774bfda12/Team5-Project3-normalize-and-export-source.Rmd) in an attempt to only show how we would work directly with an existing relational database within this file.  

### Reading the Normalized Dataset

We used [DuckDB](https://duckdb.org/), a popular open-source embeddable database to manage our relational database. We chose DuckDB as way to try working with a different type of database within R and for its portability â€” though we don't recommend storing sensitive data in a public repository as we have for this exercise. 

After loading the __duckdb__ package, we initialized DuckDB by establishing a connection and importing our pre-normalized database parquet files. Next we used SQL to join the tables into a single dataframe.


```{r init-db-connection}

# 1. make an in-memory db and store the connection in a variable
con <- dbConnect(duckdb::duckdb())

# 2. read parquet files and register them as tables to the database
tables <- c('jobs','skills','job_skills','companies')
for (t in tables) {
  parquet_file <- paste('datasets/',t,'.parquet', sep="")
  duckdb_register(con, t, read_parquet(parquet_file))
}

# 3. join the tables with SQL and save as dataframe
full_df <- dbGetQuery(con,"WITH skills_by_id AS (
                    SELECT job_id,
                      STRING_AGG (skill_name, ',' ) AS skills
                    FROM job_skills AS x
                    LEFT JOIN skills AS s 
                      ON s.skill_id = x.skill_id 
                    GROUP BY job_id
                  )

                  SELECT j.*, c.company, s.skills
                  FROM jobs AS j
                  LEFT JOIN skills_by_id AS s
                      ON s.job_id = j.job_id 
                  LEFT JOIN companies AS c
                    ON c.company_id = j.company_id
            ")

glimpse(full_df)

```

### Data Tidying


```{r}

# 1. Standardize Column Names
full_df <- full_df %>%
  rename_all(~str_to_lower(.)) %>%    # Convert all column names to lowercase
  rename_all(~str_replace_all(., " ", "_")) # Replace spaces with underscores

# 2. Remove Duplicates
full_df <- full_df %>%
  distinct()  # Keep only unique rows

# 3. Handle Missing Values
# Replace NA values with 'Unknown' in character columns
full_df <- full_df %>%
  mutate(across(where(is.character), ~replace_na(., "Unknown")))

# Display the cleaned data frame
head(full_df) #the head() command produces a huge output in the HTML, what do you think about using glimpse()

```


```{r}

# Assuming 'full_df' contains job postings and skill information

# 1. Separate the skills column back into individual rows, renaming it to "skill_name":
skill_counts <- full_df %>%
  separate_rows(skills, sep = ",") %>%    # Split skills by commas into individual rows
  rename(skill_name = skills) %>%         # Rename the column to "skill_name"
  mutate(skill_name = str_trim(skill_name)) %>%  # Remove extra spaces around skill names
  count(skill_name, sort = TRUE) %>%      # Count occurrences of each skill and sort by frequency
  rename(frequency = n)                   # Rename count column to 'frequency'

# 2. Display the top skills
head(skill_counts, 10)  # Show the top 10 skills by frequency
```

```{r}

# Plot the top 10 skills
ggplot(skill_counts %>% head(10), aes(x = reorder(skill_name, -frequency), y = frequency)) +
  geom_bar(stat = "identity") +
  labs(title = "Top 10 Skills in Data Science Job Postings",
       x = "Skill",
       y = "Frequency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}

# List of text columns to normalize
text_columns <- c("job_title", "job_summary", "job_location", "search_position", "company")

# Apply normalization to each text column
full_df <- full_df %>%
  mutate(across(all_of(text_columns), ~ str_trim(tolower(.))))

# Display the first few rows of the updated dataset
head(full_df) #the head() command produces a huge output in the HTML, what do you think about using glimpse()

```

### Data Analysis
Diving deeper into analysis, we can check to see if the top 10 skills are influenced by the job level. Our data contains two values for the `job_level` field, 'Mid senior' and 'Associate.' We will take the same approach we did for the `skill_counts` data frame and make sure to include `job_level` in our output. Once we create the `skill_by_level` data frame, we will use it to merge two subsets filtered for each job title and combined through the row name. Doing so produces the table seen below.
```{r skill by level}

#Skill frequency by job level
skill_by_level <- full_df %>%
  separate_rows(skills, sep = ",") %>%    # Split skills by commas into individual rows
  rename(skill_name = skills) %>%         # Rename the column to "skill_name"
  mutate(skill_name = str_trim(skill_name)) %>%  # Remove extra spaces around skill names
  group_by(skill_name,job_level) %>% 
  count(job_level, sort = TRUE) %>%      # Count occurrences of each skill and sort by frequency
  rename(frequency = n)

#Comparison of top 10 skills based on job level
job_level_comparison <- merge(
  skill_by_level %>% 
    filter(job_level=='Mid senior') %>% 
    head(10),
  skill_by_level %>% 
    filter(job_level=='Associate') %>% 
    head(10),
  by='row.names'
) %>% 
  arrange(desc(frequency.x)) %>% 
  rename(top_mid_senior_skill = skill_name.x,
         top_associate_skill = skill_name.y) %>% 
  select(2,5)

print("Top 10 Skills by Job Level:")
job_level_comparison

```

The above data frame illustrates the slight differences in skills that exist between 'Mid senior' and 'Associate' level positions. Below, are a few more interesting statistics regarding the job titles, companies, and regions within our data set. The first one tabulates the top job titles within our data set which seems to be filled by mostly 'Mid senior,' engineer and analytics positions. The next one gives a glimpse into which companies have the most amount of job postings. Interestingly enough, a bit of research comes to show that the majority of these are recruiting companies. Last, but not least, we have a distribution of the regions with the most job postings. Out of the 10 listed, the majority of them fall on the east coast United Sates, save California, Texas, the UK, and Illinois.
```{r other distributions}

#Distribution of top job titles, companies, and regions
job_title_distribution <- full_df %>%
  count(job_title, sort = TRUE) %>%
  head(10)  # Top 10 job titles

company_distribution <- full_df %>%
  count(company, sort = TRUE) %>%
  head(10)  # Top 10 companies

regional_distribution <- full_df %>%
  mutate(region=str_replace(job_location,'(.*?),(.*?)', '\\2')) %>% 
  count(region, sort = TRUE) %>%
  head(10)  # Top 10 regions

print("Top 10 Job Titles by Frequency:")
print(job_title_distribution)

print("Top 10 Companies by Frequency:")
print(company_distribution)

print("Top 10 Regions by Frequency:")
print(regional_distribution)

```

## Conclusions / Findings and Recommendations





<small><sup>*</sup> Our team made an attempt to scrape current data from Indeed, but were unable to replicate the procedure. For demonstrative purposes, we did (scrape data](https://github.com/mcastro64/D607_Project3/blob/bcdc0748160cd2e9abca7ece7aff03f774bfda12/Team5-Project3-scraper.Rmd) from a UK-based jobs engine, which could be used to compare the differences between the two in a version of this future project</small>
